NPFL103 Information Retrieval: Assignment 2
===========================================

Lecturer: Pavel Pecina <pecina@ufal.mff.cuni.cz>
Year: 2021/22

Contents of this document

 1. Introduction
 2. Goal and objectives
 3. Specification
 4. Package contents
 5. File formats
 6. Evaluation
 7. Submission
 8. Notes
 9. Grading
10. Plagiarism and joined work
11. Terms of use

1. Introduction 

This is the second assignment of the Information Retrieval (NPFL103) course at
the Faculty of Mathematics and Physics, Charles University.

Course details are available here: https://ufal.mff.cuni.cz/courses/npfl103

2.1 Goal

To learn abbout available frameworks for Information Retrieval and use them to
deliver state-of-the-art results on the provided test collections (the same as
in the Assignment 1).

2.2 Objectives

 - Learn about publicly available information retrieval frameworks and choose
   one of them (without any restrictions)
 - Use the selected framework to setup/implement an IR system.
 - Optimize the system on the provided test collections (using the training
   topics)
 - Write a detailed report on your experiments.
 - Present your results during the course practicals.

3. Detailed specification

 A) Learn about publicly available information retrieval frameworks, for example:

     * Anserini: http://anserini.io/
     * Dragon: http://dragon.ischool.drexel.edu/
     * Lemur (Indri): http://www.lemurproject.org/
     * Lucene: http://lucene.apache.org
     * Lucy: https://attic.apache.org/projects/lucy.html
     * MeTA: https://meta-toolkit.org/
     * Sphinx: http://sphinxsearch.com/
     * Terrier: http://terrier.org/
     * Wumpus: http://stefan.buettcher.org/cs/wumpus/
     * Xapian/Omega: https://xapian.org/
     * Zettair: http://www.seg.rmit.edu.au/zettair/
     
       ... (any other)

 B) Choose one, install it and setup a system to index the test collections.

 C) First, design and evaluate a baseline system as similar as possible to the
    baseline system from Assignment 1:

    run-0 (baseline):
    - terms: forms
    - case normalization: no
    - removing stopwords: no
    - query construction: all forms from "title"
    - term weighting: natural
    - document frequency weighting: none
    - vector normalization: cosine
    - similarity measurement: cosine
    - relevance feedback: none
    - query expansion: none
    
    Use the provided Czech and English test collections and generate results
    for the training and test topics for both the languages.

    You are exected to provide the following four files:
    - run-0_train_cs.res
    - run-0_test_cs.res
    - run-0_train_en.res
    - run-0_test_en.res

    Include at most 1000 top-ranked documents for each topic.

    Evaluate the results for training topics using the trec_eval tool.
    
 D) Tune the system by employing alternative/more advanced methods/techniques
    and select the best combination/configuration (denoted as run-1) which
    optimizes the system's performance on the set of training topics (use Mean
    Average Precision as the main evaluation measure) for each of the two
    languages.
    
    You are allowed to use any third-party text processing/annotation tool
    (e.g. MorphoDiTa, available from http://ufal.mff.cuni.cz/morphodita, which
    usefull for lemmatzation and available for both Czech and English). You may
    use different approaches for Czech and for English (e.g. lemmatization for
    Czech, stemming for English).
    
    The only constraints are: i) the queries are constructed automatically
    as words from the topic titles only (i.e. you cannot use the topic
    descriptions and topic narratives to construct the queries) ii) retrieval
    is completely automatic (no user intervention is allowed).
    
    run-1 (constrained):
    - terms: ???
    - case normalization: ???
    - removing stopwords: ???
    - query construction: automatic from titles only
    - term weighting: ???
    - document frequency weighting: ???
    - vector normalization: ???
    - similarity measurement: ???
    - relevance feedback: only pseudo-relevance-based techniques are allowed
    - query expansion: ???

    Generate result files of this system for the training topics and test
    topics for both Czech and English. Include at most 1000 top-ranked
    documents for each topic.

    You are exected to provide the following four files:
    - run-1_train_cs.res
    - run-1_test_cs.res
    - run-1_train_en.res
    - run-1_test_en.res
    
 E) Optionally, you can submit another system (denoted as run-2) with
    absolutely no restrictions. You can use modified queries and use external
    data resources (e.g. thesauri).
 
    run-2 (unconstrained):
    - terms: ???
    - case normalization: ???
    - removing stopwords: ???
    - query construction: ???
    - term weighting: ???
    - document frequency weighting: ???
    - vector normalization: ???
    - similarity measurement: ???
    - relevance feedback: ???
    - query expansion: ???
    
    Again, generate result files for the training topics and test topics.
    Include at most 1000 top-ranked documents for each topic.

 F) Write a detailed report (in Czech/Slovak or English) describing details of
    your system (including building instructions, if needed), all the submitted
    runs (including instructions how the result files were generated) all
    conducted experiments and report their results on the training topics.
    Discuss the results and your achievements, compare them with those obtained
    in Assignment 1. Compare the results and approaches for Czech and English.
    
    For all experiments, report "map" (Mean Average Precision) as the main
    evaluation measure and "P_10" (Precision of the 10 first documents) as the
    secodary measure. For run-0 and run-1 with the training topics (both Czech
    and English) plot the Averaged 11-point averaged precision/recall graphs and
    include them in the report. 
    
 G) Prepare a short presentation (a few slides, 5-10 minutes) summarizing your
    approach, employed data structures, conducted experiments, results,
    decisions, unsolved issues etc., to be presented to the lecturer and
    your classmates during the practicals.  
    
4. Package contents

   With the only exception of this README file, the contents of this package is
   the same as for Assignment 1.

 - documents_cs -- a directory containing 221 xml files with the total of 81735
     documents in Czech (see format description in Sec 5.1)

 - documents_en -- a directory containing 365 xml files with the total of 88110
     documents in English (see format description in Sec 5.1)
   
 - documents_cs.lst -- a list of 221 filenames containing the Czech documents
 - documents_en.lst -- a list of 365 filenames containing the English documents

 - qrels-train_cs.xml -- manually assessed relevance judgements for the Czech
     training topics (format description in Sec 5.4)
 - qrels-train_en.xml -- manually assessed relevance judgements for the English
     training topics (format description in Sec 5.4)
 - qrels-test_cs.xml -- manually assessed relevance judgements for the Czech
     test topics (not provided to students)
 - qrels-test_en.xml -- manually assessed relevance judgements for the English
     test topics (not provided to students)

 - topics-train_cs.xml -- specification of the training topics in Czech
 - topics-train_en.xml -- specification of the training topics in English
 - topics-test_cs.xml -- specification of the test topics in Czech
 - topics-test_en.xml -- specification of the test topics in Czech

 - topics-test.lst  -- identifiers of the training topics
 - topics-train.lst -- identifiers of the test topics

 - sample-results.res -- example of result file (see Sec 5.3)

 - trec_eval-9.0.7.tar.gz -- the source code of the evaluation tool (see the
   included README for building instructions)

 - README -- this file

5. File formats

5.1 Document format

The document format uses a labeled bracketing, expressed in the style of SGML.
The SGML DTD used for verification at TREC/CLEF is included in the archive. The
philosophy in the formatting has been to preserve as much of the original
structure as possible, but to provide enough consistency to allow simple
decoding of the data.

Every document is bracketed by <DOC> </DOC> tags and has a unique document
number, bracketed by <DOCNO> </DOCNO> tags. The set of tags varies depending
on the language (see the DTD for more details). The Czech documents typically
contain the following tags (with corresponding end tags):
   
  <DOC>
  <DOCID>
  <DOCNO>
  <DATE>
  <GEOGRAPHY>
  <TITLE>
  <HEADING>
  <TEXT>

The English tag set is much richer than Czech. Generally all the textual
content (in any type of brackets) can be indexed, however this is also
something to (try to) optimize. 

5.2 Topic format

Topic specification uses the following SGML markup:

  <top lang="xx">
  <num>
  ...
  </num>
  <title>
  ...
  </title>
  <desc>
  ...
  </desc>
  <narr>
  ...
  </narr>
  </top>

5.3 TREC result format 

The format of the system results (.res files) is 5 tab-separated columns:

  1. qid
  2. iter
  3. docno
  4. rank
  5. sim
  6. run_id

Example:
  10.2452/401-AH 0 LN-20020201065 0 0 baseline
  10.2452/401-AH 0 LN-20020102011 1 0 baseline

The important fields are "qid" (query ID, a string), "docno" (document number,
a string appearing in the DOCNO tags in the documents), "rank" (integer
starting from 0), "sim" (similarity score, a float) and "run_id" (identifying
the system/run name, must be same for one file). The "sim" field is ignore by
the evaluation tool but you are required to fill it.  The "iter" (string) field
is ignored and unimportant for this assignment.

5.4 Query relevance format 

Relevance for each "docno" to "qid" is determined from train.qrels, which
contains 4 columns:
  1. qid
  2. iter
  3. docno
  4. rel

The "qid", "iter", and "docno" fields are as described above, and "rel" is
a Boolean (0 or 1) indicating whether the document is a relevant response for
the query (1) or not (1).

Example:
 10.2452/401-AH 0 LN-20020201065 1
 10.2452/401-AH 0 LN-20020201066 0

 Document LN-20020201065 is relevant to topic 10.2452/401-AH.
 Document LN-20020201066 is not relevant to topic 10.2452/401-AH.
 
6. Evaluation

The evaluation tool is provided in the "trec_eval-9.0.7.tar.gz" package.
Consult the "README" file inside the package for detailed instructions. 

Evaluation is performed by executing:
 ./eval/trec_eval -M1000 train-qrels.dat sample.res 

 where
 -M1000 specifies the maximum number of documents per query (do not change
 this parameter).
 
The tool outputs a summary of evaluation statistics:

  runid -- run_id
  num_q -- number of queries
  num_ret -- number of returned documents
  num_rel -- number of relevant documents
  num_rel_ret -- number of returned relevant documents
  map -- mean average precision (this is the main evaluation measure).
  ...
  iprec_at_recall_0.00 -- Interpolated Recall - Precision Averages at 0.00 recall
  ...
  P_10 -- Precision of the 10 first documents

For details see  http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf

There are also alternative implementations of the trec_eval tool available from
various sources. You can use them on your own risk. The official evaluation
will be done by the official TREC implementation included in the package.

7. Submission

Submission will be done by email. You will need a single (zip, tgz) package
containing:
  - the source code of your system and a "README" file with instructions how to
    build/setup/configure your system  and how to run the experiments.
  - The result files for training and test topics for at least run-0 and run-1
    for English and for Czech.
  - The "report.pdf" file with your written report.
  - The "slides.pdf" file with a few slides you will use for presentation
    during the practicals.

8. Notes

This assignment follows the Assignment 1 with the only difference that you are
required to use third-party (publicly available) IR framework instead of
developing your own.

9. Grading

You can earn up to 100 points: 

  0-65 points for the implementation of the system, experiments, results on the
       training topics, and the report.  
  0-30 points for the performance of the constrained system (run-1) on the test
       topics   
   0-5 points for the oral presentation

10. Plagiarism and joined work

No plagiarism will be tolerated. The assignment is to be worked on on your own.
All cases of confirmed plagiarism will be reported to the Student Department.
You are only allowed to share your performance scores on the training data.

11. Terms of use

The data in this package (documents, topics, relevance assessment) may not be
distributed and/or shared in any way and can be used only for the purpose of
completing the assignments of the NPFL103 Information Retrieval.
